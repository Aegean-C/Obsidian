#! https://zhuanlan.zhihu.com/p/645580192

# 【机器学习】线性模型(1) - 线性回归

![](线性回归.png)

# 1 基本形式

给定数据集
$$
D=\{(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)\}\\
$$
其中 $x_i=(x_{i1},x_{i2},...,x_{id})$ 表示由 $d$ 个属性描述的样本，$x_{ij}$ 表示第 $i$ 个样本在第 $j$ 个属性上的取值，$y_i \in \mathbb{R}.$

**线性回归**（linear regression）试图学得  
$$
\begin{aligned} &f(x)=w^Tx+b \\ &s.t.\quad f(x_i) \simeq y_i, \quad i =1,2,...,n \\ \end{aligned} \\
$$
其中 $w=(w_1,w_2,..,w_d)^T.$
我们可以将 $b$ 吸入 $w$ ，得到简单形式：  
$$
\begin{aligned} &f(x)=w^Tx \\ &s.t.\quad f(x_i) \simeq y_i, \quad i =1,2,...,n \\ \end{aligned} \\
$$
其中 $w=(b,w_1,w_2,...,w_d)^T,\ x_i=(1,x_{i1},x_{i2},...,x_{id})^T.$

# 2 参数估计

## 2.1 最小二乘法（LSE）

### 2.1.1 思想

**均方误差**（也叫平方损失）是回归任务中最常用的性能度量。基于均方误差最小化来进行模型求解的方法称为“**最小二乘法**”（Least Square Method，简称**LSE**）。

### 2.1.2 优化目标

采用均方误差定义的损失函数如下：
$$
\begin{aligned}L(w)&=\sum_{i=1}^n||f(x_i)-y_i||^2_2 \\&=\sum_{i=1}^n||w^Tx_i-y_i||^2_2 \\\end{aligned} \\
$$
我们记  
$$
X=(x_1, x_2, ..., x_n)^T \\ y=(y_1, y_2, ..., y_n)^T \\
$$

则损失函数可以转化为如下形式：
$$
L(w)=||Xw-y||_2^2 \\
$$
因此，线性回归模型的最小二乘“参数估计”（parameter estimation）即求解最优化问题：
$$
\begin{aligned} \hat{w} &= \arg\min_{w}L(w) \\ &= \arg \min_{w}||Xw-y||_2^2 \\ \end{aligned} \\
$$

### 2.1.3 推导

展开目标函数得到：
$$
\begin{aligned}L(w)&=||Xw-y||_2^2 \\ &=(Xw-y)^T(Xw-y) \\&=w^TX^TXw-y^TXw-w^TX^Ty+y^Ty \\&=w^TX^TXw-2w^TX^Ty+y^Ty \\\end{aligned} \\
$$
对 $w$ 求导得到：
$$
\frac{\partial L(w)}{\partial w}=2X^TXw-2X^Ty \\
$$
令导数等于零可以得到 $w$ 最优解的闭式解（解析解）：
$$
\hat{w}=(X^TX)^{-1}X^Ty \\
$$
上式中，$(X^TX)^{-1}X^T$ 被称为伪逆，记作 $X^+$ 。对于满秩的 $X$，可以直接求解。但是对于非满秩的样本集合，需要对 $X$ 进行奇异值分解（SVD），得到：
$$
X=U\Sigma V^T \\
$$
于是有
$$
X^+=V\Sigma^{-1}U^T \\
$$

### 2.1.4 几何意义

① 均方误差对应了常用的欧几里得距离或简称“欧氏距离”（Euclidean distance）。在线性回归中，最小二乘法就是使得所有样本的模型预测值（一条直线）预测值和真实值的欧式距离之和最小。

② 样本集合 $X$ 的列向量是 $d$ 个 $n$ 维向量，因此可以张成一个 $n$ 维空间的 $p$ 维子空间 $(p<min(n,d))$ ，模型可以写成 $f(X)=Xw$, 也就是 $X$ 的列向量的某种线性组合，而真实值向量 $y$ 是 $n$  维向量，不一定能被 $X$ 的列向量线性表出（嵌入到该空间中），均方误差就对应了 $f(X)$ 和 $y$ 两个向量之间的欧式距离。最小二乘法就是使得该距离最小，误差的最小值就是向量 $y$ 到该子空间的投影距离，满足如下条件时达到：
$$
X^T(Xw-y)=0 \\
$$
因此，
$$
\hat{w}=(X^TX)^{-1}X^Ty \\
$$

## 2.2 极大似然估计（MLE）

### 2.2.1 思想

基于极大似然原理来进行参数估计的方法称为“**极大似然估计**”（Maximum Likelihood Estimate，简称**MLE**）。

### 2.2.2 优化目标

似然函数为
$$
L(w)=\prod_{i=1}^{n}p(y_i|x_i,w) \\
$$
极大似然估计就是求解如下的最优化问题：
$$
\begin{aligned} \hat{w} &= \arg\max_{w}L(w) \\ &= \arg \max_{w}\prod_{i=1}^{n}p(y_i|x_i,w) \\ \end{aligned} \\
$$

### 2.2.3 推导

在线性回归中，假设真实值和预测值的误差 $\epsilon$ 服从均值为零，标准差为 $\sigma$ 的高斯分布，即 $\epsilon\sim{N(0,\sigma^2)}$ , 那么 $y=w^Tx+\epsilon\sim{N(w^Tx, \sigma^2)}$ .

带入似然函数，那么最优化问题可以转化为：
$$
\begin{aligned}\hat{w} &= \arg\max_{w}L(w) \\ &= \arg \max_{w}\prod_{i=1}^{n}p(y_i|x_i,w) \\ &=\arg\max_{w}\prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}} \\ &=\arg \max_{w}\sum_{i=1}^nlog(\frac{1}{\sqrt{2\pi\sigma}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2}}) \\ &=\arg \max_{w}(-\frac{1}{2\sigma^2}\sum_{i=1}^n(y_i-w^Tx_i)^2) \\ &=\arg\min_{w}\sum_{i=1}^n(y_i-w^Tx_i)^2 \\ &= \arg \min_{w}||Xw-y||_2^2\end{aligned} \\
$$
可以看到，其于最小二乘法的最优化问题是一致的。

### 2.2.4 与LSE的联系

**LSE** 等价于 **噪声为高斯分布的MLE** 。

## 2.3 最大后验概率（MAP）

### 2.2.1 思想

假设 $w$ 存在一个先验分布 $p(w)$，那么 $w$ 就可以看作贝叶斯统计中的一个随机变量，根据贝叶斯定理，可以得到 $w$ 的后验概率分布为
$$
p(w|y)=\frac{p(y|w)p(w)}{p(y)} \\
$$
“**最大后验概率**”（maximum a posteriori probability，简称**MAP**）估计就是估计参数 $w$ 为这个后验分布的**众数**（最大值）。

### 2.2.2 优化目标

后验概率为
$$
\begin{aligned}p(w|y,X)&=\frac{p(y|w,X)p(w)}{p(y|X)} \end{aligned} \\
$$
最大后验概率估计就是求解如下的最优化问题：
$$
\begin{aligned} \hat{w} &= \arg\max_{w}p(w|y,X) \\ &= \arg \max_{w}p(y|w,X)p(w) \\ \end{aligned} \\
$$

### 2.2.3 推导

① 假设误差 $\epsilon$ 服从均值为零，标准差为 $\sigma_0$ 的高斯分布；$w_i$ 服从均值为零，方差为 $\sigma_1$ 的高斯分布，即
$$
\epsilon\sim{N(0,\sigma_0^2)}, \qquad w_i\sim{N(0,\sigma_1^2)} \\
$$
带入后验概率，那么最优化问题可以转化为：
$$
\begin{aligned}\hat{w} &= \arg\max_{w}p(w|y,X) \\ &= \arg \max_{w}p(y|w,X)p(w) \\ &=\arg\max_{w}[\prod_{i=1}^np(y_i|w,x_i)]p(w) \\ &=\arg \max_{w}[\sum_{i=1}^nlog(\frac{1}{\sqrt{2\pi\sigma_0}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2_0}})+log(\frac{1}{\sqrt{2\pi\sigma_1}}e^{-\frac{w^Tw}{2\sigma^2_1}})] \\ &=\arg \max_{w}[-\frac{1}{2\sigma_0^2}\sum_{i=1}^n(y_i-w^Tx_i)^2-\frac{w^Tw}{2\sigma_1^2}] \\ &=\arg\min_{w}[\sum_{i=1}^n(y_i-w^Tx_i)^2+\frac{\sigma_0^2}{\sigma_1^2}w^Tw] \\ &= \arg \min_{w}[||Xw-y||_2^2+\frac{\sigma_0^2}{\sigma_1^2}w^Tw] \\ \end{aligned} \\
$$
可以看到，其与经过L2正则化的最小二乘法的最优化问题是一致的。

② 假设误差 $\epsilon$ 服从均值为零，标准差为 $\sigma_0$ 的高斯分布；$w_i$ 服从位置参数为零，尺度参数为 $b$ 的Laplace分布，即
$$
\epsilon\sim{N(0,\sigma_0^2)}, \qquad w_i\sim{Laplace(0,b)} \\
$$
带入后验概率，那么最优化问题可以转化为：
$$
\begin{aligned}\hat{w} &= \arg\max_{w}p(w|y,X) \\ &= \arg \max_{w}p(y|w,X)p(w) \\ &=\arg\max_{w}[\prod_{i=1}^np(y_i|w,x_i)]p(w) \\ &=\arg \max_{w}[\sum_{i=1}^nlog(\frac{1}{\sqrt{2\pi\sigma_0}}e^{-\frac{(y_i-w^Tx_i)^2}{2\sigma^2_0}})+log(\frac{1}{2b}e^{-\frac{||w||_1}{b}})] \\ &=\arg \max_{w}[-\frac{1}{2\sigma_0^2}\sum_{i=1}^n(y_i-w^Tx_i)^2-\frac{||w||_1}{2b}] \\ &=\arg\min_{w}[\sum_{i=1}^n(y_i-w^Tx_i)^2+\frac{\sigma_0^2}{b}||w||_1] \\ &= \arg \min_{w}[||Xw-y||_2^2+\frac{\sigma_0^2}{b}||w||_1] \\ \end{aligned} \\
$$
可以看到，其与经过L1正则化的最小二乘法的最优化问题是一致的。

### 2.2.4 与LSE的联系

**L1正则化的LSE** 等价于 **噪声为高斯分布，权重先验为Laplace分布的MAP** 。

# 3 正则化

在实际应用时，如果样本数不远远大于样本的特征维度，很可能造成过拟合，为了缓解过拟合问题，我们有以下三种解决方案：

- 增加数据，提高样本数
- 特征选择（降低特征维度）如PCA算法
- 引入正则化项

正则化可理解为一种“罚函数法”，即对不希望得到的结果施以惩罚，从而使得优化过程趋向于希望目标。从贝叶斯估计的角度来看，正则化项可认为是提供了模型的先验概率。正则化一般是在损失函数（如上面介绍的平方损失）上加入正则化项（表示模型的复杂度对模型的惩罚）。

$L_p$ 范数（norm）是常用的正则化项，其中 $L_2$ 范数 $||w||_2$ 倾向于 $w$ 的分量取值尽量均衡，即非零分量个数尽量稠密，而 $L_0$ 范数 $||w||_0$ 和 $L_1$ 范数 $||w||_1$ 则倾向于 $w$ 的分量尽量稀疏，即非零分量个数尽量少。（ $L_0$ 范数 $||w||_0$ 等于 $w$ 非零分量的个数）

## 3.1 L1正则化 —— Lasso 回归

### 3.1.1 思想

**Lasso回归**（Least Absolute Shrinkage and Selection Operator Regression）是一种线性回归的变体，它在模型训练过程中加入了L1正则化项，以实现特征选择和模型参数的稀疏化。Lasso回归被广泛用于高维数据集上的特征选择，可以帮助剔除对预测目标影响较小的特征，从而提高模型的泛化能力。

### 3.1.2 优化目标

Lasso回归的优化目标是最小化以下损失函数：
$$
L(w) = ||Xw-y||^2_2 + \lambda||w||_1 \\
$$
其中，

- $y$ 表示观测目标（输出）向量；
- $X$ 是数据矩阵，每一行代表一个样本，每一列代表一个特征；
- $w$ 是回归系数向量，代表模型的参数；
- $\lambda$ 是正则化参数，用于控制正则化项的强度。

损失函数由两部分组成：第一部分是普通的平方损失函数，用于拟合目标变量；第二部分是L1正则化项，用于对回归系数进行惩罚。正则化参数 $\lambda$ 控制了正则化项的权重。当 $\lambda$ 为0时，Lasso回归退化为普通的线性回归，没有特征选择的作用；当 $\lambda$ 较大时，Lasso回归会倾向于将某些回归系数压缩为0，实现特征选择的效果。

### 3.1.3 效果

L1正则化项在损失函数中具有稀疏化效果，即它使得许多回归系数为零，从而将对应的特征从模型中剔除，只保留对预测目标有显著影响的特征。这使得Lasso回归特别适用于高维数据集，可以帮助简化模型，减少过拟合的风险。

事实上，对 $w$ 施加“稀疏约束”（即希望 $w$ 的非零分量尽可能少）最自然的是使用 $L_0$ 范数，但 $L_0$ 范数不连续，难以优化求解，因此常使用 $L_1$ 范数来近似。 

需要注意的是，Lasso回归对于某些数据集和特征情况下可能存在系数完全收缩为零的情况，这意味着该特征被完全剔除，可能导致信息丢失。因此，在实际应用中，需要通过交叉验证等方法来选择合适的正则化参数 $\lambda$ ，以平衡模型的拟合能力和泛化能力。

## 3.2 L2正则化 —— 岭回归

### 3.2.1 思想

**岭回归**（Ridge Regression），也称为L2正则化回归，是一种线性回归的改进方法，用于解决多重共线性问题和提高模型的泛化能力。岭回归在模型训练过程中加入了L2正则化项，通过惩罚回归系数的平方和，从而对模型参数进行约束和稳定。

### 3.2.2 优化目标

岭回归的优化目标是最小化以下损失函数：
$$
L(w) = ||Xw-y||^2_2 + \lambda||w||_2^2 \\
$$
其中，

- $y$ 表示观测目标（输出）向量；
- $X$ 是数据矩阵，每一行代表一个样本，每一列代表一个特征；
- $w$ 是回归系数向量，代表模型的参数；
- $\lambda$ 是正则化参数，用于控制正则化项的强度。

损失函数由两部分组成：第一部分是普通的平方损失函数，用于拟合目标变量；第二部分是L1正则化项，用于对回归系数进行惩罚。正则化参数 $\lambda$ 控制了正则化项的权重。

### 3.2.3 推导

展开目标函数得到：
$$
\begin{aligned}L(w)&=||Xw-y||_2^2+\lambda||w||_2^2 \\&=w^TX^TXw-2w^TX^Ty+y^Ty+\lambda w^Tw \\\end{aligned} \\
$$
对 $w$ 求导得到：
$$
\frac{\partial L(w)}{\partial w}=2X^TXw-2X^Ty+2\lambda w \\
$$
令导数等于零可以得到 $w$ 最优解的闭式解（解析解）：
$$
\hat{w}=(X^TX+\lambda\mathbb{I})^{-1}X^Ty \\
$$
因此，通过选择合适的 $\lambda$ 可以避免 $X^TX$ 不可逆的问题。 

### 3.2.4 效果

L2正则化项在损失函数中对回归系数的平方和进行了惩罚，它使得回归系数向量的每个元素都趋向于缩小，但不会完全收缩为零。相比于L1正则化（Lasso回归），L2正则化对回归系数的收缩更加平滑，不会导致特征完全被剔除，而是将其权重调整到较小的值。

$L_1$ 范数和 $L_2$ 范数正则化都有助于降低过拟合风险，但前者还会带来额外的好处：它比后者更易于获得“稀疏”（sparse）解，即它求得的 $w$ 会有更少的非零分量。我们从以下两个角度理解这一点：

- 从最小化损失的角度看，由于 $L_1$ 范数求导在 $0$ 附近的左右导数都不是 $0$ ，因此更容易取到 $0$ 解。
- 从几何的角度看，最优解需要在平方损失项和正则化项之间折中，即出现在平方损失项等值线和正则化项等值线相交处。线性回归的平方损失函数等值线在 $w$ 空间是一个椭球族，从下图可以看出 $L_1$ 范数正则化项等值线和平方损失项等值线交点常出现在坐标轴上，而 $L_2$ 范数正则化项等值线和平方损失项等值线焦点常出现在某个象限中。因此， $L1$ 范数比 $L_2$ 范数更易得到稀疏解。
![2维w空间的平方损失、L1范数和L2范数等值线](2维w空间的平方损失、L1范数和L2范数等值线.png)

岭回归特别适用于解决多重共线性问题，多重共线性是指输入特征之间存在较高的线性相关性（也就是 $X$ 不是满秩的），可能导致回归系数估计不稳定。通过加入L2正则化项，岭回归可以缓解多重共线性的影响，提高模型的稳定性。

类似于Lasso回归，在实际应用中，需要通过交叉验证等方法来选择合适的正则化参数  $\lambda$ ，以平衡模型的拟合能力和泛化能力。

# 4 特点

机器学习分为频率派和贝叶斯派两个派系：频率派代表统计机器学习，贝叶斯派代表概率图模型，线性回归模型在统计机器学习中占据核心地位，具有以下几个特点：
- **线性**：线性又分为属性线性、全局线性和系数线性。全局线性是指没有对预测值进行非线性激活，使其输出为非线性
- **全局性**：对样本空间整体进行建模
- **数据未加工**：线性回归在建模钱并未对数据进行加工处理

若将线性回归模型的某些特点打破，则可以衍生出其他模型，代表如下：
- 属性非线性：特征转换（多项式回归）
- 全局非线性：线性分类（激活函数是非线性的）
- 系数非线性：神经网络、感知机
- 非全局性：线性样条回归（分段函数）、决策树
- 数据加工：PCA、流形

# 参考

[1] 周志华. 机器学习. 北京：清华大学出版社，2016.  

[2] https://www.yuque.com/bystander-wg876/yc5f72/mkn2fh  

[3] https://www.bilibili.com/video/BV1aE411o7qd?p=12&vd_source=057e09c98bd04bfd54edb6489683bbdb  

[4] https://zh.wikipedia.org/wiki/%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%88%86%E5%B8%83  
