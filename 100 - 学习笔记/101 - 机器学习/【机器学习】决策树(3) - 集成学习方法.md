# 1 简介

## 1.1 定义

**集成学习（Ensemble Learning）** 通过构建并结合多个学习器来完成学习任务。

## 1.2 别名

- 多分类器系统（multi-classifier system）
- 基于委员会的学习（committee-based learning）

## 1.3 分类

### 1.3.1 按个体学习器类型分类

- **同质（homogeneous）集成：** 集成中的个体学习器都是同种类型的个体学习器，这样的集成是同质的。同质集成中的个体学习器称为**基学习器(base learner)** ，相应的学习算法称为**基学习算法(base learning algorithm)** 。
- **异质（heterogenous）集成：** 集成中包含不同类型的个体学习器，这样的集成是异质的。异质集成中的个体学习器被称为**组件学习器(component learner)** 。

### 1.3.2 按个体学习器生成方式分类

- **序列化方法：** 个体学习器间存在强依赖关系、必须串行生成。代表算法：Boosting。
- **并行化方法：** 个体学习器间不存在强依赖关系、可同时生成。代表算法：Bagging。

# 2 集成学习方法

## 2.1 Bagging

### 2.1.1 思想

**Bagging** （**B**ootstrap **agg**regat**ing**) 是并行式集成学习方法最著名的代表，它直接基于自助采样法进行采样集的构造。

给定包含 $m$ 个样本的数据集，进行 $m$ 次自助采样（有放回采样），可以得到含有 $m$ 个样本的采样集。通过这种方式，我们可采样出 $T$ 个含有 $m$ 个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。

![](../../400%20-%20Resource/Pasted%20image%2020230903193608.png)

### 2.1.2 特点

初始训练集中有的样本在采样集里多次出现，有的则从未出现。可以做一个简单的估计，样本在 $m$ 次采样中始终不被采到的概率是 $(1-\frac{1}{m})^m$， 取极限得到
$$
\lim_{m->\infty}(1-\frac{1}{m})^m=\frac{1}{e}\approx0.368 \\
$$

可以看出每个基学习器只使用了初始训练集中约有 63.2% 的样本，因此剩下约36.8%的样本可用作验证集来对泛化性能进行”包外估计“（out-of-bag estimate）。集成对样本的包外预测为：
$$
H^{oob}(x)=arg\max_{y\in\mathcal{Y}}\sum_{t=1}^{T}\mathbb{I}(h_t(x)=y)·\mathbb{I}(x\notin D_{t}) \\
$$
假定基学习器的计算复杂度为 $O(m)$ ，则 Bagging 的复杂度大致为 $T(O(m) +O(s))$ ，考虑到采样与投票/平均过程的复杂度$O(s)$ 很小，而 $T$  

### 2.1.1 流程

给定包含 m 个样本的数据集，我们经过 m 次有放回采样，得到一个含有 m 个样本的采样集。初始训练集中约有 63.2% 的样本出现在采样集中。 



# 4 理论分析

## 4.1 集成的好处

## 4.2 集成泛化性能更好的条件

在一般经验中，如果把好坏不等的东西掺到一起，那么通常结果会是比最坏的要好一些，比最好的要坏一些。集成学习把多个学习器结合起来，要想获得比最好的单一学习器更好的性能，个体学习器应“好而不同”，即满足以下两个条件：

- **准确性** 
个体学习器要具有一定的准确性，不能太坏，至少不差于弱学习器。弱学习器通常指泛化性能略优于随机猜测的学习器，例如在二分类问题上精度略大于50%的分类器。

- **多样性** 
个体学习器要具有一定的差异，即误差是相互独立的。

假设基分类器的错误率相互独立，则由Hoeffding不等式可知，集成的错误率为
$$
	\begin{aligned} P(H(x)\ne f(x))&=\sum_{k=0}^{\left[ T/2 \right]}C_{T}^{k}(1-\epsilon)^k\epsilon^{T-k} \\ &\leq e^{-\frac{1}{2}T(1-2\epsilon)^2} \\ \end{aligned} \\
$$

其中，$H(X)$ 表示集成分类，$f(x)$ 表示真实分类， $T$ 表示个体分类器个数， $\epsilon$ 表示单个个体分类器的错误率。

从上式可以看出，随着个体分类器数目 $T$ 的增大，集成的错误率将指数级下降，最终趋向于零。并且，个体分类器错误率 $\epsilon$ 越小，下降的速度越快，使用的个体学习器越少。

然而，在现实任务中，个体学习器是为解决同一问题训练出来的，它们的错误率显然不可能相互独立。一般来说，个体学习器的准确性很高以后，要增加多样性就需牺牲准确性。集成学习研究的核心就是如何产生并结合“好而不同”的个体学习器。

## 4.3 误差 - 分歧分解

## 4.4 多样性度量

## 4.5 多样性增强