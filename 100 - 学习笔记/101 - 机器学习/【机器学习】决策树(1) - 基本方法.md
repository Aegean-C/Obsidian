## 1 基本流程

　　决策树（Decision Tree）是一种常见的机器学习方法，它易于理解、可解释性强，即可以作为分类模型，也可以作为回归模型。

一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点。叶结点对应于决策结果，其他每个结点则对应于一个属性测试。每个结点包含的样本集合根据属性测试的结果被划分到子结点中。根结点包含样本全集。

从根结点到每个叶结点的路径对应了一个判定测试序列。决策树学习的目的是为了产生一棵泛化能力强（即处理未见示例能力强）的决策树，其基本流程遵循简单且直观的“分而治之”（divide-and-conquer）策略。

决策树的生成是一个递归过程。在决策树基本算法中，有三种情形会导致递归返回：

① **情形：**当前结点包含的样本全属于同一类别，无需划分。

**处理操作：**把当前结点标记为叶结点并将其类别设定为该类别。

② **情形：**当前属性集为空，或者所有样本在所有属性上取值相同，无法划分。

**处理操作：**把当前结点标记为叶结点并将其类别设定为该结点所含样本最多的类别。（该操作利用了当前结点的后验分布）

③ **情形：**当前结点包含的样本集合为空，不能划分。

**处理操作：**把当前结点标记为叶结点并将其类别设定为其**父节点**所含样本最多的类别。（该操作把父节点的样本分布作为当前结点的先验分布）

## 2 划分选择

　　决策树学习的关键是如何选择最优划分属性。一般而言，随着划分过程不断进行，我们希望决策树的分支结点所包含的样本尽可能属于同一类别，即结点的“纯度”（purity）越来越高。我们主要介绍一下三种划分标准。

### 2.1 信息增益

**2.1.1 信息熵**

“信息熵”（information entropy）是度量样本集合纯度最常用的一种指标。其定义为：

�(�)=−∑�=1�|��||�|���2|��||�|

其中， �� 表示集合 D 中属于第 k 类样本的样本子集。

**2.1.2 条件熵**

针对某个离散特征 A，对于数据集 D 的条件熵 �(�|�) 为：

�(�|�)=∑�=1�|��||�|�(��)=−∑�=1�|��||�|(∑�=1�|���||��|���2|���||��|)

其中 �� 表示 D 中特征 A 取第 v 个值的样本子集， ��� 表示 �� 中属于第 k 类的样本子集。

**2.1.3 信息增益**

于是可以计算出使用特征A对样本集D进行划分所获得的“信息增益”（information gain），其定义为：信息增益 = 信息熵 - 条件熵

����(�,�)=�(�)−�(�|�)

一般而言，信息增益越大，则意味着使用特征 A 来划分所获得的“纯度提升”越大。ID3算法就是使用信息增益来进行决策树的划分属性选择。

### 2.2 信息增益率

　　信息增益准则对可取值数目较多的特征有所偏好，类似“编号”的特征其信息增益接近于 1。为了克服这一缺点，C4.5引入信息增益率作为准则，在信息增益的基础上增加特征的固有值作为惩罚项。信息增益率定义为：

����_�����(�,�)=����(�,�)��(�)��(�)=−∑�=1�|��||�|���2|��||�|

��(�) 称为特征 A 的固有值（intrinsic value）。特征A取值数目越多（即V越大），则��(�) 的值通常会越大。

　　信息增益率准则克服了信息增益准则偏好可取值数据数据较多特征的缺点，但是其对可取值数据较少的特征有所偏好。因此C4.5算法并不是直接选择信息增益率最大的候选划分特征，而是使用一个启发式方法：先从候选划分特征中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

### 2.3 基尼指数

**2.3.1 基尼值**

　　熵模型拥有大量耗时的对数运算，“基尼指数”（Gini index）在简化模型的同时还保留了熵模型的优点。数据集 D 的纯度可用基尼值来度量：

����(�)=∑�=1�|��||�|(1−|��||�|)=1−∑�=1�(|��||�|)2

直观来说，Gini(D)反映了**从数据集D中随机抽取两个样本，其类别标记不一致的概率**。因此，Gini(D)越小，则数据集D的纯度越高，这和信息熵一致。

**2.3.2 基尼指数**

类似条件熵，特征A的基尼指数定义为：

����(�|�)=∑�=1�|��||�|����(��)=1−∑�=1�|��||�|∑�=1�(|���||��|)2

基尼指数偏向于特征值较多的特征，类似信息增益(条件熵）。基尼指数可以用来度量任何不均匀分布，是介于 0~1 之间的数，0 是完全相等，1 是完全不相等。

一般而言，基尼指数越小，则意味着使用特征 A 来划分所获得的“纯度提升”越大。CART算法就是使用基尼指数来进行决策树的划分属性选择。

**2.3.3 基尼指数与熵模型的比较**

　　对比基尼指数的定义和条件熵的定义，我们发现，其形式上仅有一个平方运算和对数运算的差异，相比熵模型，基尼指数运算更加简单，其性能也和熵模型比较接近。

既然基尼指数与熵模型性能接近，但到底与熵模型的差距有多大呢？

我们知道 ��(�)=−1+�+�(�) ，所以

�(�)=−∑�=1���ln⁡��≈1−∑�=1���2

因此，基尼指数可以理解为熵模型的一阶泰勒展开。对于二类分类，基尼系数、半熵和分类误差率关系曲线如下：

![](https://pic1.zhimg.com/80/v2-c8ab3041d65152c7429f925e5c85cd9c_720w.webp)

## 3 剪枝处理

　　剪枝（pruning）是决策树学习算法对付“过拟合”的主要手段。在决策树学习中，为了尽可能正确分类训练样本，结点划分过程不断重复，有时会造成决策树分支过多，这时就可能导致过拟合。因此，可通过主动去掉一些分支来降低过拟合的风险。

　　决策树剪枝的基本策略有“预剪枝“（prepruning）和“后剪枝”（postpruning）。

### 3.1 预剪枝

　　预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点。

　　预剪枝是一种早停策略，决策树中的**早停策略**主要有：

　　① 当前结点包含的样本数或者样本类别数低于某一个阈值；

　　② 当前节点所包含的特征集合为空或者包含的样本在所有特征上取值相同；

　　③ 当前结点的划分不能带来决策树泛化性能提升。

　　如何判断决策树泛化性能是否提升，可以采用留出法，预留一部分数据用作“验证集”以进行性能评估。在训练集上生成决策树，在验证集上剪枝。

　　我们把仅有一层划分的决策树，称为“**决策树桩**”（decision stump)。

　　**预剪枝的优点：**预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。

　　**预剪枝的缺点：**有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高；预剪枝基于“贪心”策略禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险。

### 3.2 后剪枝

　　后剪枝是指先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换成叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点。

　　如果某个结点后剪枝前后验证集精度并无提高，虽然剪枝对决策树泛化性能并无改变，但根据奥卡姆剃刀准则，剪枝后的模型更好。因此，实际的决策树算法在此种情形下通常要进行剪枝。

　　**后剪枝的优点：**后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。

　　**后剪枝的缺点：**后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多。

## 4 连续与缺失值

### 4.1 连续值处理

　　由于连续特征的可取值数目不再有限，因此不能直接根据连续特征的可取值来对结点进行划分。此时，连续属性离散化技术可派上用场。最简单的策略是采用二分法（bi-partition）对连续属性进行处理。

　　给定样本集D和连续特征A，假定A在D上出现了V个不同的取值，将这些值从小到大进行排序，记为 {�1,�2,...,��} 。基于划分点 t 可将D分为子集 ��− 和 ��+ ，其中��− 包含那些在特征A上取值不大于t的样本，而��+ 则包含那些在特征A上取值大于t的样本。显然，对相邻的属性取值 �� 和 ��+1 来说，t在区间 ）[��,��+1） 中取任意值所产生的划分结果相同。因此，对连续特征A，我们可考察包含 V -1 个元素的候选划分点集合

��={��+��+12|1≤�≤�−1}即把 ）[��,��+1） 的中位点作为候选划分点（也可以选择在训练集中出现的不大于中位点的最大值，即 �� 。这可以保证最终决策树使用的划分点都在训练集中出现过）。然后，我们就可以像离散属性值一样来考察这些划分点，选取最优的划分点进行样本集合的划分。例如，对离散信息增益稍加改造：

����(�,�)=max�∈������(�,�,�)=�(�)−max�∈��∑�∈{−,+}|���||�|�(���)其中， ����(�,�,�) 是样本集 D 基于划分点 t 二分后的信息增益。于是，我们就可以选择使����(�,�,�)最大化的划分点。

　　与离散特征不同，若当前结点划分特征为连续特征，该特征还可以作为其后代结点的划分特征。

### 4.2 缺失值处理

　　决策树处理缺失值可以分为两个子问题：

① 如何在特征值缺失的情况下进行划分特征选择？

② 给定划分特征，若样本在该特征上的值缺失，如何对样本进行划分？

　　对问题①，给定训练集D和特征A，令 �~ 表示D中在特征A上没有缺失值的样本子集。我们可以仅根据�~ 来判断特征A的优劣。令 �~� 表示 �~ 中在特征A上取值为第v个值的样本子集，�~� 表示 �~ 中属于第k类的样本子集。假定我们为每个样本 x 赋予一个权重 �� (在决策树学习开始阶段，根结点中各样本权重初始化为1),并定义：�=∑�∈�~��∑�∈����~�=∑�∈�~���∑�∈�~���~�=∑�∈�~���∑�∈�~��　　直观地看，对特征Ａ， � 表示无缺失值样本所占的比例， �~� 表示无缺失值样本中第 k 类所占的比例， �~� 则表示无缺失值样本中在特征A上取第 v 个值的样本所占的比例。

　　基于上述定义，我们可以将信息增益推广为：

����(�,�)=�×����(�~,�)=�×(�(�~)−∑�=1��~��(�~�))　　对于问题②，若样本 x 在划分特征A上的取值已知，则将 x 划入与其取值对应的子结点，且样本权值在子结点中保持为 �� 。若样本 x 在划分特征A上的取值未知，则将 x 同时划入所有子结点，且样本权值调整为 �~�⋅�� ; 直观地看，这就是让同一样本以不同的概率划入到不同的子结点中去。

## 5 多变量决策树

　　若我们把每个特征视为坐标空间中的一个坐标轴，则d个特征描述的样本就对应了d维空间中的一个数据点，对样本分类则意味着在这个坐标空间中寻找不同类样本之间的分类边界。

　　决策树所形成的分类边界有一个明显的特点：轴平行（axis-parallel），即它的分类边界由若干个与坐标轴平行的分段组成。这样的分类边界使得学习结果有较好的可解释性，因为每一段划分都直接对应了某个特征取值。但在学习任务的真实分类边界比较复杂时，必须使用很多段划分才能获得较好的近似，此时的决策树会相当复杂，由于要进行大量的特征测试，预测时间开销会很大。

　　若能使用斜的划分边界，则决策树模型将大为简化。“多变量决策树”（multivariate decision tree）就是能实现这样的“斜划分”甚至更复杂划分的决策树。以实现斜划分的多变量决策树为例，在此类决策树中，非叶结点不再是仅对某个特征，而是对特征的线性组合进行测试；换言之，每个非叶结点是一个形如 ∑�=1�����=� 的线性分类器，其中 �� 是特征 �� 的权重， �� 和 t 可在该结点所含的样本集和特征集上学得。于是，与传统的“单变量决策树”（univariate decision tree）不同，在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分特征，而是试图建立一个合适的线性分类器。

## 6 参考

[1] 周志华. 机器学习. 北京：清华大学出版社，2016.

[2] [阿泽：【机器学习】决策树（上）——ID3、C4.5、CART（非常详细）](https://zhuanlan.zhihu.com/p/85731206)